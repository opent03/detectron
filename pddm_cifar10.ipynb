{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from baselines.cifar10_loader import load_and_process_cifar\n",
    "from models.classifier import MLP\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "#torch.set_float32_matmul_precision('medium')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoolDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.labels[idx]\n",
    "        l = torch.zeros(size=(10,))\n",
    "        l[i] = 1\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long).squeeze(-1)#l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA explained variance ratio:  1.0\n"
     ]
    }
   ],
   "source": [
    "dct = load_and_process_cifar(n_components=20, return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_val, x_test, x_test_ood =  dct['x_train'], dct['x_val'], dct['x_test'], dct['x_test_ood']\n",
    "y_train, y_val, y_test, y_test_ood =  dct['y_train'], dct['y_val'], dct['y_test'], dct['y_test_ood']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MLP(input_size=20,\n",
    "    hidden_layers=[32, 32],\n",
    "    output_size=10,\n",
    "    dropout=0.5,\n",
    "    logger='auc', loss='ce',\n",
    "    loss_params=None, optim='adam',\n",
    "    optim_params=dict(lr=1e-4, weight_decay=0),\n",
    "    scheduler=None,\n",
    "    scheduler_params=None,\n",
    "    legacy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HPU available: False, using: 0 HPUs\n",
      "/home/viet/anaconda3/envs/dl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    limit_train_batches=100,\n",
    "    max_epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(CoolDataset(x_train, y_train), batch_size=batch_size, num_workers=20)\n",
    "val_loader = DataLoader(CoolDataset(x_val, y_val), batch_size=batch_size, num_workers=20)\n",
    "test_loader = DataLoader(CoolDataset(x_test, y_test), batch_size=batch_size, num_workers=20)\n",
    "ood_test_loader = DataLoader(CoolDataset(x_test_ood, y_test_ood), batch_size=batch_size, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | Sequential       | 2.1 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "2 | _logger | AUCLogger        | 0      | train\n",
      "-----------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6491712d59e343f789f85e34ec085614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22ae01d7b8e40b08d3d5adf6d71e4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viet/anaconda3/envs/dl/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e285ab9f1b4dacb08a66740f7af45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d0ceb1ae6c4374961fa689d59e0a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc9dae3022d467593993e345abbf3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0de324250e4f6794585167af3fdfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad474e445404248921b1baba718c852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab09f693b2a43c1ab2b53afbe44f733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f66897cce4492eb18accda212ee682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5159a599e54b14b78589db6e237150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8f9fcba23b47c19a9a84a610059dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08408fc49e17471fbfbc3c0c9867f7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c7823bb92d4a0ebb7a10767366a272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5966e5389e44da915f6a80dd09bf49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b48ab292846495ba79af5d310bd55dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01bf3e453b24cf885fc8ea0f4328419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3095f420778d4c1baa30db1c22483899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031899d355dd4f108ef49141f56c9689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15165495963445181ee185c66dfb14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3751ed0464bb4b2f8e24284dff7efc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af15201b49b04633907f13aec0ba052e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b7f5c2cc0d454da7d50487537118fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(base_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint('pddm_cifar10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLP.load_from_checkpoint('pddm_cifar10.pth', input_size=20,\n",
    "    hidden_layers=[32, 32],\n",
    "    output_size=10,\n",
    "    dropout=0.5,\n",
    "    logger='auc', loss='ce',\n",
    "    loss_params=None, optim='adam',\n",
    "    optim_params=dict(lr=1e-3, weight_decay=0),\n",
    "    scheduler=None,\n",
    "    scheduler_params=None,\n",
    "    legacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    count = 0\n",
    "    acc = 0 \n",
    "    for features, labels in loader:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "        logits = model(features)\n",
    "        output = F.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(output, dim=-1)\n",
    "        acc += torch.sum(preds == labels)\n",
    "        count += len(labels)\n",
    "    return acc / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9008, device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model2, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dataset(dataset, name):\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\n",
    "    'x_train1': x_train1,\n",
    "    'x_train2': x_train2,\n",
    "    'x_val': x_val,\n",
    "    'x_test': x_test,\n",
    "    'y_train1':y_train1,\n",
    "    'y_train2': y_train2,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(dct, 'pddm_cifar10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pddm_cifar10.pkl', 'rb') as f:\n",
    "    dct = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10.1 PDDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_cifars():\n",
    "    tmp = {}\n",
    "    splits = ['train', 'val', 'test']\n",
    "    kk = ['x', 'y']\n",
    "    # load iid \n",
    "    for k in kk:\n",
    "        for split in splits:\n",
    "            with open('baselines/data/cifar10_' + k + '_' + split + '.npy', 'rb') as f:\n",
    "                tmp[k + '_' + split] = np.load(f)\n",
    "    \n",
    "    with open('baselines/data/cifar101_x_test.npy', 'rb') as f:\n",
    "                tmp['x_test_ood'] = np.load(f)\n",
    "    with open('baselines/data/cifar10.1_v6_labels.npy', 'rb') as f:\n",
    "                tmp['y_test_ood'] = np.load(f)\n",
    "    tmp['x_test_ood'], tmp['y_test_ood'] = sklearn.utils.shuffle(tmp['x_test_ood'], tmp['y_test_ood'])\n",
    "    return tmp\n",
    "    # load ood\n",
    "dct = load_cifars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels, sample_weights=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.sample_weights = sample_weights\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx], self.sample_weights[idx] if self.sample_weights is not None else torch.tensor(1)\n",
    "\n",
    "def dce_loss(logits: torch.Tensor, labels: torch.Tensor, mask: torch.Tensor, alpha=None,\n",
    "             use_random_vectors=False, weight=None) -> torch.Tensor:\n",
    "    if mask.all():\n",
    "        # if all labels are positive, then use the standard cross entropy loss\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "    if alpha is None:\n",
    "        alpha = 1 / (1 + (~mask).float().sum())\n",
    "\n",
    "    num_classes = logits.shape[1]\n",
    "\n",
    "    q_logits, q_labels = logits[~mask], labels[~mask]\n",
    "    if use_random_vectors:\n",
    "        # noinspection PyTypeChecker,PyUnresolvedReferences\n",
    "        p = - torch.log(torch.rand(device=q_labels.device, size=(len(q_labels), num_classes)))\n",
    "        p *= (1. - F.one_hot(q_labels, num_classes=num_classes))\n",
    "        p /= torch.sum(p)\n",
    "        ce_n = -(p * q_logits).sum(1) + torch.logsumexp(q_logits, dim=1)\n",
    "\n",
    "    else:\n",
    "        zero_hot = 1. - F.one_hot(q_labels, num_classes=num_classes)\n",
    "        ce_n = -(q_logits * zero_hot).sum(dim=1) / (num_classes - 1) + torch.logsumexp(q_logits, dim=1)\n",
    "\n",
    "    if torch.isinf(ce_n).any() or torch.isnan(ce_n).any():\n",
    "        raise RuntimeError('NaN or Infinite loss encountered for ce-q')\n",
    "\n",
    "    if (~mask).all():\n",
    "        return (ce_n * alpha).mean()\n",
    "\n",
    "    p_logits, p_labels = logits[mask], labels[mask]\n",
    "    ce_p = F.cross_entropy(p_logits, p_labels, reduction='none', weight=weight)\n",
    "    return torch.cat([ce_n * alpha, ce_p]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_dim=20, hid_dim=32, out_dim=10, p=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.p = p\n",
    "    \n",
    "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
    "        self.fc2 = nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        self.fc3 = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.dropout(F.relu(self.fc1(x)), p=self.p)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), p=self.p)\n",
    "        return self.fc3(x)\n",
    "\n",
    "class MLP_Model:\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, p, batch_size, train_epochs, learning_rate, alpha=2):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        #print('Running neural nets on {}'.format(self.device))\n",
    "        self.net = Net(in_dim, hid_dim, out_dim, p)\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.train_epochs = train_epochs\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "        self.criterion = dce_loss\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def train_disagreement(self, x, y, sample_weights=None):\n",
    "        model = self.net\n",
    "        model.train()\n",
    "        train_loader = self._create_dataloader(x, y, sample_weights=sample_weights)\n",
    "        for e in tqdm(range(epochs)):\n",
    "            for features, labels, sample_weights in train_loader:\n",
    "                features, labels, sample_weights = features.to(device), labels.to(device), sample_weights.to(device)\n",
    "                output = model(features)\n",
    "                self.optimizer.zero_grad()\n",
    "                mask = (sample_weights == torch.ones_like(sample_weights)).squeeze(-1)\n",
    "                loss = self.criterion(output, labels, mask, self.alpha)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def train_base_model(self, x, y):\n",
    "        model = self.net\n",
    "        model.train()\n",
    "        train_loader = self._create_dataloader(x, y)\n",
    "        for e in tqdm(range(epochs)):\n",
    "            for features, labels, sample_weights in train_loader:\n",
    "                features, labels, sample_weights = features.to(device), labels.to(device), sample_weights.to(device)\n",
    "                output = model(features)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = nn.CrossEntropyLoss()(output, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "    def evaluate_model(self, x, y):\n",
    "        model = self.net\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loader = self._create_dataloader(x, y)\n",
    "        with torch.no_grad():\n",
    "            for features, labels, sample_weights in loader:\n",
    "                features, labels, sample_weights = features.to(device), labels.to(device), sample_weights.to(device)\n",
    "                output = F.softmax(model(features), dim=1)\n",
    "                preds = torch.argmax(output, dim=1)\n",
    "                correct += torch.sum(preds == labels).item()\n",
    "                total += len(labels)\n",
    "                \n",
    "        return correct/total\n",
    "            \n",
    "        \n",
    "    def _create_dataloader(self, features, labels, sample_weights=None):\n",
    "        features, labels = torch.as_tensor(features, dtype=torch.float32), torch.as_tensor(labels)\n",
    "        if sample_weights is not None:\n",
    "            sample_weights = torch.as_tensor(sample_weights)\n",
    "        dataset = CustomDataset(features, labels, sample_weights=sample_weights)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "        return dataloader\n",
    "    \n",
    "    def get_labels(self, x):\n",
    "        self.net.eval()\n",
    "        probas = F.softmax(self.net(x), dim=1)\n",
    "        preds = torch.argmax(probas, dim=1)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, x_test_ood =  dct['x_train'], dct['x_val'], dct['x_test'], dct['x_test_ood']\n",
    "y_train, y_val, y_test, y_test_ood =  dct['y_train'], dct['y_val'], dct['y_test'], dct['y_test_ood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(x_train, y_train)\n",
    "valset = CustomDataset(x_val, y_val)\n",
    "testset = CustomDataset(x_test, y_test)\n",
    "testset_ood = CustomDataset(x_test_ood, y_test_ood, torch.ones(size=y_test_ood.shape) * alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=20)\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=True, num_workers=20)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True, num_workers=20)\n",
    "test_ood_loader = DataLoader(testset_ood, batch_size=64, shuffle=True, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "base_model = MLP_Model(20,32, 10, 0.2, 64, 10, 1e-4)\n",
    "base_model.train_base_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.evaluate_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_preds = base_model.get_labels(torch.as_tensor(x_val, dtype=torch.float32).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_merged = np.concatenate([x_train, x_val], axis=0)\n",
    "y_merged = np.concatenate([y_train, base_preds], axis=0)\n",
    "sample_weights = np.ones(shape=(len(x_train) + len(x_val), 1))\n",
    "sample_weights[len(x_train):] = sample_weights[len(x_train):] * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net = MLP_Model(20,32, 10, 0.2, 64, 10, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(new_net.net.parameters(), lr=1e-4)\n",
    "def _create_dataloader(features, labels, sample_weights=None):\n",
    "        features, labels = torch.as_tensor(features, dtype=torch.float32), torch.as_tensor(labels)\n",
    "        if sample_weights is not None:\n",
    "            sample_weights = torch.as_tensor(sample_weights)\n",
    "        dataset = CustomDataset(features, labels, sample_weights=sample_weights)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True)\n",
    "        return dataloader\n",
    "    \n",
    "def train_disagreement(model, x, y, optimizer, sample_weights=None, alpha=10):\n",
    "        model.train()\n",
    "        train_loader = _create_dataloader(x, y, sample_weights=sample_weights)\n",
    "        for e in tqdm(range(epochs)):\n",
    "            for features, labels, sample_weights in train_loader:\n",
    "                features, labels, sample_weights = features.to(device), labels.to(device), sample_weights.to(device)\n",
    "                output = model(features)\n",
    "                optimizer.zero_grad()\n",
    "                mask = (sample_weights == torch.ones_like(sample_weights)).squeeze(-1)\n",
    "                loss = dce_loss(output, labels, mask, alpha)\n",
    "                loss.backward()\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64905 0.5721\n"
     ]
    }
   ],
   "source": [
    "new_net = MLP_Model(20,32, 10, 0.2, 64, 10, 1e-4)\n",
    "optimizer = optim.AdamW(new_net.net.parameters(), lr=1e-4)\n",
    "train_disagreement(new_net.net, x_merged, y_merged, optimizer, sample_weights, 30)\n",
    "a = new_net.evaluate_model(x_train, y_train)\n",
    "b = new_net.evaluate_model(x_val, y_val)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo1(base_model:MLP_Model, data, rounds=100, alpha=30):\n",
    "    x_train, x_val, y_train, y_val = data\n",
    "    base_preds = base_model.get_labels(torch.as_tensor(x_val, dtype=torch.float32).cuda())\n",
    "    Phi = []\n",
    "    for i in tqdm(range(rounds)):\n",
    "        new_model = MLP_Model(20,32, 4, 0.2, 64, 10, 1e-4, alpha=alpha)\n",
    "        id_val = np.random.choice(len(x_val), len(x_val), replace=True)\n",
    "        xx, yy = x_val[id_val], base_preds[id_val]\n",
    "        x_merged = np.concatenate([x_train, xx], axis=0)\n",
    "        y_merged = np.concatenate([y_train, yy], axis=0)\n",
    "        #x_merged = np.concatenate([x_train, x_val], axis=0)\n",
    "        #y_merged = np.concatenate([y_train, base_preds], axis=0)\n",
    "        \n",
    "        sample_weights = np.ones(shape=(len(x_train) + len(xx), 1))\n",
    "        sample_weights[len(x_train):] = sample_weights[len(x_train):] * 2\n",
    "        new_model.train_disagreement(x_merged, y_merged, sample_weights)\n",
    "        dis = 1 - ((new_model.get_labels(torch.as_tensor(x_val, dtype=torch.float32).cuda()) == yy).sum() / len(x_val))\n",
    "        print(dis)\n",
    "        Phi.append(dis)\n",
    "    return Phi\n",
    "\n",
    "def algo2(base_model, data, Phi, alpha=30): \n",
    "    (x_train, x_val, y_train, y_val) = data\n",
    "    base_preds = base_model.get_labels(torch.as_tensor(x_val, dtype=torch.float32).cuda())\n",
    "    \n",
    "    new_model = MLP_Model(20,32, 4, 0.2, 64, 10, 1e-4, alpha=alpha)\n",
    "    id_val = np.random.choice(len(x_val), len(x_val), replace=True)\n",
    "    xx, yy = x_val[id_val], base_preds[id_val]\n",
    "    #x_merged = np.concatenate([x_train, xx], axis=0)\n",
    "    #y_merged = np.concatenate([y_train, yy], axis=0)\n",
    "    x_merged = np.concatenate([x_train, x_val], axis=0)\n",
    "    y_merged = np.concatenate([y_train, base_preds], axis=0)\n",
    "    \n",
    "    sample_weights = np.ones(shape=(len(x_train) + len(xx), 1))\n",
    "    sample_weights[len(x_train):] = sample_weights[len(x_train):] * 2\n",
    "    new_model.train_disagreement(x_merged, y_merged, sample_weights)\n",
    "    dis = 1 - ((new_model.get_labels(torch.as_tensor(x_val, dtype=torch.float32).cuda()) == yy).sum() / len(x_val))\n",
    "    print(dis)\n",
    "    return dis\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [1,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [4,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [5,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [10,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [11,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [12,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Phi \u001b[38;5;241m=\u001b[39m \u001b[43malgo1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36malgo1\u001b[0;34m(base_model, data, rounds, alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x_train) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(xx), \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m sample_weights[\u001b[38;5;28mlen\u001b[39m(x_train):] \u001b[38;5;241m=\u001b[39m sample_weights[\u001b[38;5;28mlen\u001b[39m(x_train):] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mnew_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_disagreement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_merged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m dis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ((new_model\u001b[38;5;241m.\u001b[39mget_labels(torch\u001b[38;5;241m.\u001b[39mas_tensor(x_val, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcuda()) \u001b[38;5;241m==\u001b[39m yy)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(x_val))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(dis)\n",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mMLP_Model.train_disagreement\u001b[0;34m(self, x, y, sample_weights)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m mask \u001b[38;5;241m=\u001b[39m (sample_weights \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(sample_weights))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mdce_loss\u001b[0;34m(logits, labels, mask, alpha, use_random_vectors, weight)\u001b[0m\n\u001b[1;32m     30\u001b[0m     ce_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(p \u001b[38;5;241m*\u001b[39m q_logits)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogsumexp(q_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     zero_hot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     ce_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(q_logits \u001b[38;5;241m*\u001b[39m zero_hot)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (num_classes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogsumexp(q_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(ce_n)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(ce_n)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/_tensor.py:941\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "Phi = algo1(base_model, (x_train, x_val, y_train, y_val), rounds=10, alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8995"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo2(base_model, (x_train, x_val, y_train, y_val), Phi, alpha=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7f8ca0171160>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Phi)\n",
    "plt.axvline(0.901)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
